- [ ] [F10] Understand Problems and Technical Discussion
- [ ] [F11] Set up Local Machine Environment
  - [ ] Install Docker: hosting localstack
  - [ ] Install Python
  - [ ] Install Terraform
  - [ ] Output 1: hosting localstack.
  - [ ] Output 2: Create Lambda function/dynamodb/s3/IAM/... by terraform, count number of row in csv file, write the result into dynamodb {"row": 3}.
- [ ] [F12] CICD Set up with resources
  - [ ] Hosting Jenkin CICD
  - [ ] Integrate Github Action (preferred)
  - [ ] Output: run CI for [F11]: validate terraform, validate python code
- [ ] [F20] Ingesting data from API and push into Kinesis Data Stream
  - [ ] Create REST API with Mock Data: Web, Report (define RAW) - Heroku Db (batching)
  - [ ] Create Kinesis Data Stream / Crawler
  - [ ] Create Lambda function to push data into Kinesis Data Stream
- [ ] [F21] Parsing and Flatting data and Send to Delivery Stream
  - [ ] Parse events in Kinesis Data Stream
  - [ ] First Normalize events into flatten data object
  - [ ] Error handling ( step function)
- [ ] [F22] Storing Data into Lake- house
  - [ ] Create Firehose Delivery Stream (Buffer) to store flatten data as Parquet format S3 bucket - column stored - compressed
  - [ ] Create Table Catalog for data in Data- lake
  - [ ] Deadline: 17 Mar, 2022
  - [ ] End Project
- [ ] [F30] Transforming Data with Slowly Changing Dimension
  - [ ] Create Dimensional model with flatten data into fact and dim tables
  - [ ] Story Fact into S3 bucket, Dim data into Dynamodb
  - [ ] Dim data able to infer into Dim table with SCD1
- [ ] [F31] Enriching Data with Data Processing method as Lambda Architect
  - [ ] Integrate data pipeline with new Data source
  - [ ] Ingest new source and store into S3 bucket
  - [ ] Overwrite Dim data in Dynamodb with new Data source with define key
- [ ] [F40] Setup Cloud Warehouse and needed permission
- [ ] [F41] Creating resources pulling data as Dimensional Model to Warehouseless
- [ ] [F50] Developing data monitoring
- [ ] [F51] Developing data quality and data validation
- [ ] [F60] Set up BI tools for visualizing data
- [ ] [F61] Setup Connection to write back data to lake- house
- [ ] [F63] Exposing API for end- user
- [ ] [F70] Deploy to AWS/Azure Cloud
- [ ] [F71] Embed Machine Learning Model to Data pipeline
